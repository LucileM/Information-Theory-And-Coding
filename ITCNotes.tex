\documentclass[10pt,a4paper,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathtools}
\usepackage{color,graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{underscore}
\usepackage{todonotes}

% style
\input{style.tex}

% Shorthands
\renewcommand{\bf}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\balpha}{\boldsymbol\alpha}
\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\bdelta}{\boldsymbol\delta}
\newcommand{\btheta}{\boldsymbol\theta}
\newcommand{\bPhi}{\boldsymbol\Phi}
\newcommand{\concept}[1]{\textcolor{RoyalBlue}{#1}} 
\newcommand{\subconcept}[1]{\textcolor{PineGreen}{\textit{#1}}}

% Allows to easily edit out parts that are dispensible
\newcommand{\opt}[1]{#1}
% Modifications in order to maximize information density
\ifdefined \longversion % Nothing
\else
  % Hide optional content
  \renewcommand{\opt}[1]{}
  % Make titles smaller
  \renewcommand{\section}[1]{
    \vspace{-0.3cm}
    \begin{center}
      \color{Bittersweet}
      \hrulefill{\small~~#1~~}\hrulefill
    \end{center}
    \vspace{-0.3cm}
  }
  \renewcommand{\subsection}[1]{\section{#1}}
\fi

\pdfinfo{
  /Title (Information Theory and Coding)
  /Creator (Lucile Madoulaud)
  /Author (Lucile Madoulaud)
  /Subject (Information Theory and Coding)
  /Keywords (ITC, information, communication, theory, coding)
}

% -----------------------------------------------------------------------

\begin{document}
\title{Information Theory and Coding}

\raggedright
\footnotesize
\sffamily
\begin{multicols*}{4}

% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\opt {
  \begin{center}
  \Large{Information Theory and Coding}
  \end{center}
}

% ----------
\section{Source Coding}

\concept{Introduction}
Diagram of a general communication system.
\subconcept{Discrete sources} output of the source is in discrete time and discrete valued.
\subconcept{Source Coding} representation of information sources in bits.
\subconcept{Source Code Function} $C:U\mapsto\{0,1\}^*=\{\emptyset,0,1,00,...\}$.

\concept{Non-Singular Codes} 
A code $C$ is \subconcept{singular} if $\exists u \neq v /\ C(u)=C(v)$. A code $C$ is \subconcept{non-singular} if it is not singular.
With a code $C$ define for a positive integer $n$ :
$C^n:U^n\mapsto\{0,1\}^*$ as $C^n(u_1,u_2,...,u_n)=C(u_1)C(u_2)...C(u_n)$
$C^*:U^*\mapsto\{0,1\}^*$ as $C^*(u_1u_2...u_n)=C(u_1)C(u_2)...C(u_n)$

\concept{Uniquely Decodable Codes}
A code $C$ is said to be \subconcept{uniquals decodable} if $C^*$ is non-singular.We want our codes to be uniquals decodable.

\concept{Prefix-Free Codes} 
A sequence $u_1,... u_n$ is a \subconcept{prefix} of $v_1,...,v_n$ if $n \geqslant m /\ u_1=v_1,...,u_m=v_m$.
A code $C$ is said to be \subconcept{prefix-free} if $\forall u \neq v$ $C(u)$ is not a prefix of $C(v)$.

\subconcept{Theorem} A prefix-free code is uniquely decodable.
(In a binary-tree representation of a PF code all codewords are found on the leaves).

\concept{Kraft's Inequality for PF Codes} 
\subconcept{Theorem}
If $C$ is PF then 
$Kraftsum(C) \triangleq \sum_{u \in U }2^{-length(C(u))} \le 1$.
\subconcept{Proposition} $Kraftsum(C^n)=\left[ Kraftsum(C) \right]^n$.

\concept{Kraft's Inequality for extensions of codes} 
\subconcept{Proposition} Suppose $C:U\mapsto\{0,1\}^*$ is a non-singular code then $Kraftsum(C) = \sum_{u \in U }2^{-length(C(u))} \le 1 + max\left[ length(C(u)) \right]$

\concept{Kraft's Inequality for uniquely decodable codes}
\subconcept{Theorem} If $C$ is a uniquely decodable code then $Kraftsum(C) \le 1$.
\subconcept{Corollary} If $C$ is a uniquely decodable code then there exists a PF code $C'$ such that $length(C(u)) = length(C'(u))$.

\concept{Reverse Kraft's inequality}
\subconcept{Theorem} Given an alphabet $U$ and a function $l:u \mapsto \{0,1,2,3,...\} /\ \sum_{u \in U }2^{-length(C(u))} \le 1$ then there exist a PF code $C:U\mapsto\{0,1\}^* /\ \forall u \in U$ $length(C(u))=l(u) $ 


\concept{Sources} A source producer a sequence $u_1,u_2,u_3,...$ each $u_i \in U$ being random variables.
A \subconcept{memory-less} source is one where $u_1,u_2,...$ are independent.
A \subconcept{stationary} source is one where each $(u_i,...,u_{i+n-1})$ has the same statistics as $(u_1,...,u_n)$ for each $i$ and each $n$.
A memory-less and stationary source is equivalent to $u_1,u_2,...$ are \subconcept{independent, identically distributed (iid)}.

\concept{Expected codeword length}
$E\left[ length(C(u)) \right]$ average number of bits/letter the code uses to represent the source. We want to minimize it and $C$ to be uniquely decodable.

% ----------

\section{Entropy}
\subconcept{Lemma} $ln(z)\le z^{-1}$ with eq if $z=1$.
\subconcept{Property} $0 \le H(U) \le log |U|$
\concept{Entropy as a lower-bound to the expected codeword length} 
\subconcept{Theorem} For any uniquely decodable code $C$ for a source $U$, we have $E\left[ length(C(u)) \right] \ge \sum_{u}p(u)log_2 \frac{1}{p(u)} \triangleq H(u)$

\concept{Existence of PF codes with average length at most entropy + 1}
\subconcept{Theorem} Given source $U$ there exists a PF code $C$ s.t. 
$E\left[ length(C(u)) \right] \ge H(u)+1$
 
\concept{Entropy of multiple random variables} 
\subconcept{Property} Suppose $U$ and $V$ are ind. RV.
Then $H(UV)=H(U)+H(V)$.
\subconcept{Observe} Suppose we have $U_1U_2...$ iid. If we use a code $C$ to represent $n$ letters at time., we will have $H(U_1...U_n) \le E\left[ length(C(U_1...U_n)) \right] \le H(U_1...U_n)+1$.
\subconcept{Also} $\frac{1}{n} H(U_1...U_n) = H(U_1)$ (iid of U).

\concept{Properties of optimal codes} 
\subconcept{1} If $p(u) < p(v)$ then $l(u) \ge l(v)$.
\subconcept{2} In an optimal PF code there are more than 2 longest codewords. If not the longest codeword can be shortered without violating the PF condition.
\subconcept{3} Among optimal codes, there is one for the two least probable symbols are siblings.

\concept{Huffman procedure} 
Procedure to design the optimal code.
\subconcept{1} Given prob $p_1,p_2,...,p_{k-1},p_k$. Start with the two smallest prob.
\subconcept{2} Group them together as the binary descendant of a node.
\subconcept{3} Repeat until one node is left.

\concept{Equivalence of PF codes and strategy for guessing via binary questions} TODO

\concept{Interpretation of entropy as expected number of questions for guessing the random variable} TODO

% --------
\section{Mutual Information}

\concept{Conditional Entropy and Mutual Information} 
\subconcept{Conditional Entropy} $H(U|V=v)=\sum_{u}p(u|v)log \frac{1}{p(u|v)}$
$H(U|V)=\sum_{v}p(v)H(U|V=v)$.
\subconcept{Conjecture} $H(U|V) \le H(U)$.
\subconcept{Mutual Information} $I(U;V)=H(U)+H(V)-H(UV)$ is the saving in the number of questions to given $U$ by the knowledge of $V$.
\subconcept{Lemma} Suppose $W$ is an alphabet and $p$ and $q$ are two prob distribution in W. Then, $\sum_{w}p(w)log \frac{p(w)}{q(w)} \ge 0$ with eq. iff $p=q$.
\subconcept{Theorem} $I(U;V)\ge 0$ with eq iff $U$ and $V$ are independent.
\subconcept{Conditional Mutual Information} 
$I(U;V|W)=H(U|W)+H(V|W)-H(UV|W)=H(U|W)-H(U|VW)=H(V|W)-H(V|UW)$
\subconcept{Theorem} $I(U;V|W)\ge 0$ with eq. iff $U$,$V$ are independent conditional in $W$ $\equiv U - V - W$.
% find the markov chain symbol

\concept{Chain Rules for entropy and mutual information} 
\subconcept{Theorem} $H(UV)=H(U)+H(V|U)=H(V)+H(U|V)$
\subconcept{Theorem} $H(U_1...U_n)=H(U_1)+H(U_2|U_1)+...+H(U_n|U_1...U_{n-1})$
\subconcept{Theorem} $I(U_1...U_n,V)=I(U_1;V)+...+I(U_n;V|U_1...U_{n-1})$
 
\concept{Review of Markov Chain} 
Suppose $X$,$Y$,$Z$ are RVs. We can write $p(xyz)=p(x)p(y|x)p(z|xy)$.
Because $p(y|x)=\frac{p(xy)}{p(x)}$.
If $X-Y-Z$ then $p(xyz)=p(x)p(y|x)p(z|y)$. 
Suppose $U_1-U_2-...-U_n$ then $H(U_1...U_n)=H(U_1)+H(U_2|U_1)+...+H(U_n|U_{n-1})$

\concept{Data Processing Inequality} 
\subconcept{Theorem} Suppose $U-V-W$ then $I(U;W) \le I(U;V)$
\subconcept{Corollary} If $U-V-W$ then $I(U;W) \le I(V;W)$.
\subconcept{Corollary} If $U-V-W-X$ then $I(U;X) \le I(V;W)$.
$I(UV;W)=I(U;W)+I(V;W|U)=I(V;W)+I(U;W|V)$.

\concept{Entropy Rate}
Given a stochastic process $U_1,U_2...$ we define its \subconcept{entropy rate}
$H(U)=\lim_{n \to \infty} \frac{1}{n}H(u_1...u_n)$ if the limit exists.

\concept{Entropy Rate of Stationary Processes} 
\subconcept{Theorem} If $u_1,u_2,...$ is stationary process, then the entropy rate exists and $\lim_{n \to \infty} \frac{1}{n}H(u_1...u_n) = \lim_{n \to \infty}H(u_n|u_1...u_{n-1})$

\concept{Coding Theorem for Stationary Sources} 
\subconcept{Theorem} If $U_1,U_2,...$ is a stationary process with entropy rate H, then $\forall \varepsilon > 0$ there exists a source code $C_n:U^n \to \{0,1\}^*$ s.t. the average code length is less than $H+\varepsilon$ 

\concept{Fixed-to-Fixed Length Source Codes} 
Codes of type $U\to \{0,1\}^*$ or $U^n \to \{0,1\}^*$ are called \subconcept{fixed-to-variable} length codes, and all our designs have error free recovery of the source from its representation.
We want \subconcept{Fixed-to-fixed} codes $C:U^n \to \{0,1\}^k$ ($2^k$ representations), to obtain efficient codes we will give up error free recovery replace this by recovery with very small prob. of error.
The code assign binary representations only to a subset $S \subset U^n$ which ensure $Pr((u_1...u_n)\in S) \approx 1$ and $|S|\le 2^k$.

% ------------
\section{Typicality}

\concept{Typicality}
Given an alphabet $U$ and distribution $p$ in $U$.
We have a source that produces iid letters $u_i$ with distribution $p$.
We want a \subconcept{set $T_{n,\varepsilon} \subset U^n$}.

\concept{Properties of Typical Sets} 
\subconcept{1.} $Pr((u_1...u_n)\in T_{n,\varepsilon,p})\approx1$
($\ge 1 - \frac{|U|}{n\varepsilon^2 p(u)}$).
\subconcept{2.} $|T_{n,\varepsilon,p}| \le 2^{n(1+\varepsilon)H(u)}$

\concept{Asymptotic Equipartition Property}
\subconcept{Def} AEP is a general property of the output samples of a stochastic source. It is fundamental to the concept of typical set used in theories of compression.
\subconcept{Theorem} If $U_1,...U_n$ iid $\sim q$ and $(u_1...u_n) \in T_{n,\varepsilon,p}$ then $Pr((U_1...U_n)=(u_1...u_n))=\prod_{i=1}^n q(u_i)$.
\subconcept{Corollary} $Pr(((U_1...U_n)=(u_1..u_n))\in T_{n,\varepsilon,p})=2^{-n(1\pm \varepsilon)D(p||q)}2^{\pm n\varepsilon H(p)}(1 \pm \varepsilon) \circeq 2^{-nD(p||q)}$.
\subconcept{Consequently} If $(X_1,Y_1)...(X_n,Y_n)$ iid $p_X p_Y$ then $Pr(((X_1,Y_1)...(X_n,Y_n))\in T_{n,\varepsilon,p_{XY}}) \circeq 2^{-nI(X;Y)}$

\concept{Source Coding by AEP}
\subconcept{1.} Assign to every member of $T_{n,\varepsilon,p}$ a distinct element of $\{0,1\}^*$. Call this $C:T_{n,\varepsilon,p} \to \{0,1\}^*$.
\subconcept{2.} The source code is the following :Observe if $(U_1...U_n) \in T_{n,\varepsilon,p}$ represent it by 0 else by 1.

\concept{Variable-to-Fixed Length Source Codes}
$\equiv$ Dual of Fixed-to-Variable length source coding
$\equiv$ Dictionary to sed source coding
\subconcept{Idea} Given an alphabet $U$, find a dictionary $D \subset U^*$, assign $\lceil log|D| \rceil$ bit binary representation to words in $D$, and then given $U_1U_2..$, parse it into $w_1,w_2...$ of dictionary words and represent each words by its binary description.

\concept{Valid and Prefix-Free Dictionaries} 
The words of valid and PF dictionaries form the leaves of a complete dictionary tree.

\concept{Relationship between word- and letter-entropies for valid, prefix-free dictionaries}
nbr of bits/letter $=\frac{m\lceil log|D| \rceil}{length(W_1)+...+length(W_m)} \to \frac{\lceil log|D| \rceil}{E[length(W_1)]}$
\subconcept{Lemma} Suppose $Z$ is non negative integer valued RV. Then $E[Z]=\sum_{n=0}^\infty Pr[Z>n]$.
\subconcept{Observation} nbr of words = nbr of leaves in tree representation $=1+(|U|-1)$(nbr interior nodes).
% --------------
\section{Tunstall procedure}

\concept{Tunstall procedure} 
\subconcept{1.} $D=U$ with $|U|$ words and one interior node (root).
\subconcept{2.} While $|D|<M$ do :
convert the most probable word/leaf into interior node and grow $|U|$ leaves from it.

\concept{Analysis of Tunstall procedure}
\subconcept{Lemma} Suppose $D$ valid and PF dictionary and $U_1U_2...$ are iid. Then $H(W_1)=E[length(W_1)]H(U_1)$.
\subconcept{Now} nbr of bits/letter $=\frac{\lceil log|D| \rceil}{E[length(W_1)]}=\frac{\lceil log|D| \rceil}{H(W_1)}H(U_1)$.

\concept{Universal Source Coding} 
\subconcept{Lemma} If $W$ is a RV taking values in $D$ and for each $w\in D$ $q p_0 \le Pr(W=w) \le q$. Then $\lceil log|D| \rceil - log\frac{1}{p_0} \le H(W) \le \lceil log|D| \rceil$.
\subconcept{Corollary} Given $U$ and $p_U$, $\varepsilon > 0$ there exists a dictionary based code which $nbr of bits/letter \le H(U)(1+\varepsilon)$.

\concept{Lempel–Ziv method - Data compression}
\subconcept{0.} Set $D=U$
\subconcept{1.} Associate to each $w \in D$ a $\lceil log|D| \rceil$bit binary representation based on dictionry order.
\subconcept{2.} Parse the next word $w$ from the source sequence using $D$, emit the representation of $w$.
\subconcept{3.} Set $D \leftarrow (D\setminus \{w\})\cup \{wu:u \in U \}$.
\subconcept{4.} Go to 1.

\concept{Analysis of Lempel–Ziv} 
\subconcept{Technique} Compare LZ to an adversary : 
adversary knows before have $u_1u_2...$.
designs a FSM to compress this sequence.
And show LZ doas as well as the adversary.


\concept{Information-Lossless FSM Compressors}
A \subconcept{Finite-State-Machine} is 1. Set $S$ of states $|S|<\infty$.
2. Initial special state $s_0 \in S$.
3. Next state function $g:SxU \to S$.
4. $f:SxU \to \{ 0,1 \}^*$.
A \subconcept{legitimate} machine has to verify :
$\forall s \in S \forall u_1...u_m \neq v_1...v_n$ if $g(s,u_1...u_m)=g(s,v_1...v_m) \Rightarrow f(s,u_1...u_m) \neq f(s,v_1...v_n)$.
\subconcept{IL} A legitimate FSM is information-lossless.

\concept{Lower bound on the output length of an IL FSM Compressor} 
\subconcept{Fact} Suppose $m$ numbers $a_1...a_m \ge 0$ with $\sum_{i=1}^m a_i=k$.
Then, $\sum_{i=1}^m a_i log \frac{a_i}{8} \ge k log \frac{k}{8m}$ (with eq. for $a_i=\frac{k}{m}$).

\concept{LZ Compressibility of sequences} 
\subconcept{Corollary} If $u_1u_2...$ is a stationary process with entropy rate $H$ then $E[p_{LZ(U_1U_2...)}] \le H$.

\concept{Optimality of Lempel–Ziv} TODO

% -----------
\section{Channels}

\concept{Communication Channels} 
\subconcept{Def}
$C=max_{P_X} I(X;Y)$.
$P_{e,i}=Pr(U_i \neq V_i)$. 
$\bar{P_e}=\frac{1}{L} \sum_{i=1}^L Pr(U_i \neq V_i)$

\concept{Discrete Memoryless Channels} 
A channel is said to be \subconcept{memoryless} if $Pr(Y_i=y|X_i=x_i,past)=Pr(Y_i=y|X_i=x_i)$
\subconcept{Recall} $h_2(\alpha)=\alpha log( \frac{1}{\alpha})+(1-\alpha) log (\frac{1}{1-\alpha})$.
\subconcept{Theorem "Bad news"} No matter how the encoder and decoder are designed, we have $h_2(\bar{P_e})+P_e log(|U|-1) \ge \tau_s (\frac{H}{\tau_s}-\frac{C}{\tau_c})$.
\subconcept{Lemma} $h_2(\frac{1}{L} \sum_{i=1}^L p_i) \ge \frac{1}{L}\sum_{i=1}^L h_2(p_i)$

\concept{Examples of Discrete Memoryless Channels (BSC and BEC)} TODO

\concept{Transmission with or without feedback} TODO

\concept{Channel Capacity} TODO

\concept{Fano's Inequality} TODO

\concept{Converse to the Channel Coding Theorem	} TODO
			
\concept{Proof of the Channel Coding Theorem} TODO

\concept{Capacity of BSC and BEC } 
\subconcept{Binary Symmetric Channel}
$p(y|x)=
 \{
   \begin{array}{r c l}
      1-\delta , x=y \\
      \delta, else 
   \end{array}$
If we choose $p_X=(\frac{1}{2},\frac{1}{2})$


\concept{Jensen's Inequality } TODO

\concept{Concavity of Mutual Information in Input Distribution} TODO
 
\concept{KKT Conditions} TODO

\concept{KKT Conditions (cont'd)} TODO
 
\concept{Application of KKT: Capacity of Z Channel} TODO
 
\concept{Continuous Alphabet: Differential Entropy} TODO

\concept{Properties of differential entropy} TODO
 
\concept{Entropy-typical seqeuences} TODO
 
\concept{Quantization} TODO
 
\concept{Entropy of Gaussian distribution} TODO

\concept{Capacity under cost constraint} TODO
 
\concept{Capacity of AWGN} TODO
 
\concept{Converse to the channel coding theorem with cost constraint} TODO
 
\concept{Parallel Gaussian channels (water-filling)} TODO

\concept{Proof of Channel Coding Theorem for general channels via Threshold Decoding} TODO

\concept{Channel Codes} TODO
 
\concept{Minimum Distance} TODO
 
\concept{Singleton Bound} TODO
 
\concept{Sphere-packing Bound} TODO
 
\concept{Gilbert–Varshamov Bound} TODO

\concept{Linear Codes} TODO
 
\concept{Generator Matrix} TODO
 
\concept{Parity-check Matrix} TODO
 
\concept{Hamming Codes} TODO

\concept{Reed–Solomon Codes} TODO

\concept{Polar Codes} TODO

% ---------- Page break
\ifdefined \longversion % Nothing
\else \newpage
\fi



% ---------- Credits
\section{Credits}
Most content taken from the lecture notes of Emre Telatar's \href{http://ipg.epfl.ch/doku.php?id=en:courses:2015-2016:itc}{Information Theory and Coding class} at EPFL, 2015.

% ---------- Footer
\vspace{0.5cm}
\hrule
\vspace{0.5cm}
\tiny
Rendered \today. Written by Lucile Madoulaud.

\end{multicols*}
\end{document}
