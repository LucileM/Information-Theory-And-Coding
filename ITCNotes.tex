\documentclass[10pt,a4paper,landscape]{article}
\usepackage{multicol}
\usepackage{calc}
\usepackage{ifthen}
\usepackage[landscape]{geometry}
\usepackage{amsmath,amsthm,amsfonts,amssymb,mathtools}
\usepackage{color,graphicx}
\usepackage[dvipsnames]{xcolor}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{underscore}
\usepackage{todonotes}

% style
\input{style.tex}

% Shorthands
\renewcommand{\bf}[1]{\ensuremath{\mathbf{#1}}}
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\balpha}{\boldsymbol\alpha}
\newcommand{\bbeta}{\boldsymbol\beta}
\newcommand{\bdelta}{\boldsymbol\delta}
\newcommand{\btheta}{\boldsymbol\theta}
\newcommand{\bPhi}{\boldsymbol\Phi}
\newcommand{\concept}[1]{\textcolor{RoyalBlue}{#1}} 
\newcommand{\subconcept}[1]{\textcolor{PineGreen}{\textit{#1}}}

% Allows to easily edit out parts that are dispensible
\newcommand{\opt}[1]{#1}
% Modifications in order to maximize information density
\ifdefined \longversion % Nothing
\else
  % Hide optional content
  \renewcommand{\opt}[1]{}
  % Make titles smaller
  \renewcommand{\section}[1]{
    \vspace{-0.3cm}
    \begin{center}
      \color{Bittersweet}
      \hrulefill{\small~~#1~~}\hrulefill
    \end{center}
    \vspace{-0.3cm}
  }
  \renewcommand{\subsection}[1]{\section{#1}}
\fi

\pdfinfo{
  /Title (Information Theory and Coding)
  /Creator (Lucile Madoulaud)
  /Author (Lucile Madoulaud)
  /Subject (Information Theory and Coding)
  /Keywords (ITC, information, communication, theory, coding)
}

% -----------------------------------------------------------------------

\begin{document}
\title{Information Theory and Coding}

\raggedright
\footnotesize
\sffamily
\begin{multicols*}{4}

% multicol parameters
% These lengths are set only within the two main columns
%\setlength{\columnseprule}{0.25pt}
\setlength{\premulticols}{1pt}
\setlength{\postmulticols}{1pt}
\setlength{\multicolsep}{1pt}
\setlength{\columnsep}{2pt}

\opt {
  \begin{center}
  \Large{Information Theory and Coding}
  \end{center}
}

% ----------
\section{Source Coding}

\concept{Introduction}
Diagram of a general communication system.
\subconcept{Discrete sources} output of the source is in discrete time and discrete valued.
\subconcept{Source Coding} representation of information sources in bits.
\subconcept{Source Code Function} $C:U\mapsto\{0,1\}^*=\{\emptyset,0,1,00,...\}$.

\concept{Non-Singular Codes} 
A code $C$ is \subconcept{singular} if $\exists u \neq v /\ C(u)=C(v)$. A code $C$ is \subconcept{non-singular} if it is not singular.
With a code $C$ define for a positive integer $n$ :
$C^n:U^n\mapsto\{0,1\}^*$ as $C^n(u_1,u_2,...,u_n)=C(u_1)C(u_2)...C(u_n)$
$C^*:U^*\mapsto\{0,1\}^*$ as $C^*(u_1u_2...u_n)=C(u_1)C(u_2)...C(u_n)$

\concept{Uniquely Decodable Codes}
A code $C$ is said to be \subconcept{uniquals decodable} if $C^*$ is non-singular.We want our codes to be uniquals decodable.

\concept{Prefix-Free Codes} 
A sequence $u_1,... u_n$ is a \subconcept{prefix} of $v_1,...,v_n$ if $n \geqslant m /\ u_1=v_1,...,u_m=v_m$.
A code $C$ is said to be \subconcept{prefix-free} if $\forall u \neq v$ $C(u)$ is not a prefix of $C(v)$.

\subconcept{Theorem} A prefix-free code is uniquely decodable.
(In a binary-tree representation of a PF code all codewords are found on the leaves).

\concept{Kraft's Inequality for PF Codes} 
\subconcept{Theorem}
If $C$ is PF then 
$Kraftsum(C) \triangleq \sum_{u \in U }2^{-length(C(u))} \le 1$.
\subconcept{Proposition} $Kraftsum(C^n)=\left[ Kraftsum(C) \right]^n$.

\concept{Kraft's Inequality for extensions of codes} 
\subconcept{Proposition} Suppose $C:U\mapsto\{0,1\}^*$ is a non-singular code then $Kraftsum(C) = \sum_{u \in U }2^{-length(C(u))} \le 1 + max\left[ length(C(u)) \right]$

\concept{Kraft's Inequality for uniquely decodable codes}
\subconcept{Theorem} If $C$ is a uniquely decodable code then $Kraftsum(C) \le 1$.
\subconcept{Corollary} If $C$ is a uniquely decodable code then there exists a PF code $C'$ such that $length(C(u)) = length(C'(u))$.

\concept{Reverse Kraft's inequality}
\subconcept{Theorem} Given an alphabet $U$ and a function $l:u \mapsto \{0,1,2,3,...\} /\ \sum_{u \in U }2^{-length(C(u))} \le 1$ then there exist a PF code $C:U\mapsto\{0,1\}^* /\ \forall u \in U$ $length(C(u))=l(u) $ 


\concept{Sources} A source producer a sequence $u_1,u_2,u_3,...$ each $u_i \in U$ being random variables.
A \subconcept{memory-less} source is one where $u_1,u_2,...$ are independent.
A \subconcept{stationary} source is one where each $(u_i,...,u_{i+n-1})$ has the same statistics as $(u_1,...,u_n)$ for each $i$ and each $n$.
A memory-less and stationary source is equivalent to $u_1,u_2,...$ are \subconcept{independent, identically distributed (iid)}.

\concept{Expected codeword length}
$E\left[ length(C(u)) \right]$ average number of bits/letter the code uses to represent the source. We want to minimize it and $C$ to be uniquely decodable.

% ----------

\section{Entropy}
\subconcept{Lemma} $ln(z)\le z^{-1}$ with eq if $z=1$.
\subconcept{Property} $0 \le H(U) \le log |U|$
\concept{Entropy as a lower-bound to the expected codeword length} 
\subconcept{Theorem} For any uniquely decodable code $C$ for a source $U$, we have $E\left[ length(C(u)) \right] \ge \sum_{u}p(u)log_2 \frac{1}{p(u)} \triangleq H(u)$

\concept{Existence of PF codes with average length at most entropy + 1}
\subconcept{Theorem} Given source $U$ there exists a PF code $C$ s.t. 
$E\left[ length(C(u)) \right] \ge H(u)+1$
 
\concept{Entropy of multiple random variables} 
\subconcept{Property} Suppose $U$ and $V$ are ind. RV.
Then $H(UV)=H(U)+H(V)$.
\subconcept{Observe} Suppose we have $U_1U_2...$ iid. If we use a code $C$ to represent $n$ letters at time., we will have $H(U_1...U_n) \le E\left[ length(C(U_1...U_n)) \right] \le H(U_1...U_n)+1$.
\subconcept{Also} $\frac{1}{n} H(U_1...U_n) = H(U_1)$ (iid of U).

\concept{Properties of optimal codes} 
\subconcept{1} If $p(u) < p(v)$ then $l(u) \ge l(v)$.
\subconcept{2} In an optimal PF code there are more than 2 longest codewords. If not the longest codeword can be shortered without violating the PF condition.
\subconcept{3} Among optimal codes, there is one for the two least probable symbols are siblings.

\concept{Huffman procedure} 
Procedure to design the optimal code.
\subconcept{1} Given prob $p_1,p_2,...,p_{k-1},p_k$. Start with the two smallest prob.
\subconcept{2} Group them together as the binary descendant of a node.
\subconcept{3} Repeat until one node is left.

\concept{Equivalence of PF codes and strategy for guessing via binary questions} TODO

\concept{Interpretation of entropy as expected number of questions for guessing the random variable} TODO

% --------
\section{Mutual Information}

\concept{Conditional Entropy and Mutual Information} 
\subconcept{Conditional Entropy} $H(U|V=v)=\sum_{u}p(u|v)log \frac{1}{p(u|v)}$
$H(U|V)=\sum_{v}p(v)H(U|V=v)$.
\subconcept{Conjecture} $H(U|V) \le H(U)$.
\subconcept{Mutual Information} $I(U;V)=H(U)+H(V)-H(UV)$ is the saving in the number of questions to given $U$ by the knowledge of $V$.
\subconcept{Lemma} Suppose $W$ is an alphabet and $p$ and $q$ are two prob distribution in W. Then, $\sum_{w}p(w)log \frac{p(w)}{q(w)} \ge 0$ with eq. iff $p=q$.
\subconcept{Theorem} $I(U;V)\ge 0$ with eq iff $U$ and $V$ are independent.
\subconcept{Conditional Mutual Information} 
$I(U;V|W)=H(U|W)+H(V|W)-H(UV|W)=H(U|W)-H(U|VW)=H(V|W)-H(V|UW)$
\subconcept{Theorem} $I(U;V|W)\ge 0$ with eq. iff $U$,$V$ are independent conditional in $W$ $\equiv U - V - W$.
% find the markov chain symbol

\concept{Chain Rules for entropy and mutual information} 
\subconcept{Theorem} $H(UV)=H(U)+H(V|U)=H(V)+H(U|V)$
\subconcept{Theorem} $H(U_1...U_n)=H(U_1)+H(U_2|U_1)+...+H(U_n|U_1...U_{n-1})$
\subconcept{Theorem} $I(U_1...U_n,V)=I(U_1;V)+...+I(U_n;V|U_1...U_{n-1})$
 
\concept{Review of Markov Chain} 
Suppose $X$,$Y$,$Z$ are RVs. We can write $p(xyz)=p(x)p(y|x)p(z|xy)$.
Because $p(y|x)=\frac{p(xy)}{p(x)}$.
If $X-Y-Z$ then $p(xyz)=p(x)p(y|x)p(z|y)$. 
Suppose $U_1-U_2-...-U_n$ then $H(U_1...U_n)=H(U_1)+H(U_2|U_1)+...+H(U_n|U_{n-1})$

\concept{Data Processing Inequality} 
\subconcept{Theorem} Suppose $U-V-W$ then $I(U;W) \le I(U;V)$
\subconcept{Corollary} If $U-V-W$ then $I(U;W) \le I(V;W)$.
\subconcept{Corollary} If $U-V-W-X$ then $I(U;X) \le I(V;W)$.
$I(UV;W)=I(U;W)+I(V;W|U)=I(V;W)+I(U;W|V)$.

\concept{Entropy Rate}
Given a stochastic process $U_1,U_2...$ we define its \subconcept{entropy rate}
$H(U)=\lim_{n \to \infty} \frac{1}{n}H(u_1...u_n)$ if the limit exists.

\concept{Entropy Rate of Stationary Processes} 
\subconcept{Theorem} If $u_1,u_2,...$ is stationary process, then the entropy rate exists and $\lim_{n \to \infty} \frac{1}{n}H(u_1...u_n) = \lim_{n \to \infty}H(u_n|u_1...u_{n-1})$

\concept{Coding Theorem for Stationary Sources} 
\subconcept{Theorem} If $U_1,U_2,...$ is a stationary process with entropy rate H, then $\forall \varepsilon > 0$ there exists a source code $C_n:U^n \to \{0,1\}^*$ s.t. the average code length is less than $H+\varepsilon$ 

\concept{Fixed-to-Fixed Length Source Codes} 
Codes of type $U\to \{0,1\}^*$ or $U^n \to \{0,1\}^*$ are called \subconcept{fixed-to-variable} length codes, and all our designs have error free recovery of the source from its representation.
We want \subconcept{Fixed-to-fixed} codes $C:U^n \to \{0,1\}^k$ ($2^k$ representations), to obtain efficient codes we will give up error free recovery replace this by recovery with very small prob. of error.
The code assign binary representations only to a subset $S \subset U^n$ which ensure $Pr((u_1...u_n)\in S) \approx 1$ and $|S|\le 2^k$.

% ------------
\section{Typicality}

\concept{Typicality}
Given an alphabet $U$ and distribution $p$ in $U$.
We have a source that produces iid letters $u_i$ with distribution $p$.
We want a \subconcept{set $T_{n,\varepsilon} \subset U^n$}.

\concept{Properties of Typical Sets} 
\subconcept{1.} $Pr((u_1...u_n)\in T_{n,\varepsilon,p})\approx1$
($\ge 1 - \frac{|U|}{n\varepsilon^2 p(u)}$).
\subconcept{2.} $|T_{n,\varepsilon,p}| \le 2^{n(1+\varepsilon)H(u)}$

\concept{Asymptotic Equipartition Property}
\subconcept{Def} AEP is a general property of the output samples of a stochastic source. It is fundamental to the concept of typical set used in theories of compression.
\subconcept{Theorem} If $U_1,...U_n$ iid $\sim q$ and $(u_1...u_n) \in T_{n,\varepsilon,p}$ then $Pr((U_1...U_n)=(u_1...u_n))=\prod_{i=1}^n q(u_i)$.
\subconcept{Corollary} $Pr(((U_1...U_n)=(u_1..u_n))\in T_{n,\varepsilon,p})=2^{-n(1\pm \varepsilon)D(p||q)}2^{\pm n\varepsilon H(p)}(1 \pm \varepsilon) \circeq 2^{-nD(p||q)}$.
\subconcept{Consequently} If $(X_1,Y_1)...(X_n,Y_n)$ iid $p_X p_Y$ then $Pr(((X_1,Y_1)...(X_n,Y_n))\in T_{n,\varepsilon,p_{XY}}) \circeq 2^{-nI(X;Y)}$

\concept{Source Coding by AEP}
\subconcept{1.} Assign to every member of $T_{n,\varepsilon,p}$ a distinct element of $\{0,1\}^*$. Call this $C:T_{n,\varepsilon,p} \to \{0,1\}^*$.
\subconcept{2.} The source code is the following :Observe if $(U_1...U_n) \in T_{n,\varepsilon,p}$ represent it by 0 else by 1.

\concept{Variable-to-Fixed Length Source Codes}
$\equiv$ Dual of Fixed-to-Variable length source coding
$\equiv$ Dictionary to sed source coding
\subconcept{Idea} Given an alphabet $U$, find a dictionary $D \subset U^*$, assign $\lceil log|D| \rceil$ bit binary representation to words in $D$, and then given $U_1U_2..$, parse it into $w_1,w_2...$ of dictionary words and represent each words by its binary description.

\concept{Valid and Prefix-Free Dictionaries} 
The words of valid and PF dictionaries form the leaves of a complete dictionary tree.

\concept{Relationship between word- and letter-entropies for valid, prefix-free dictionaries}
nbr of bits/letter $=\frac{m\lceil log|D| \rceil}{length(W_1)+...+length(W_m)} \to \frac{\lceil log|D| \rceil}{E[length(W_1)]}$
\subconcept{Lemma} Suppose $Z$ is non negative integer valued RV. Then $E[Z]=\sum_{n=0}^\infty Pr[Z>n]$.
\subconcept{Observation} nbr of words = nbr of leaves in tree representation $=1+(|U|-1)$(nbr interior nodes).
% --------------
\section{Tunstall procedure}

\concept{Tunstall procedure} 
\subconcept{1.} $D=U$ with $|U|$ words and one interior node (root).
\subconcept{2.} While $|D|<M$ do :
convert the most probable word/leaf into interior node and grow $|U|$ leaves from it.

\concept{Analysis of Tunstall procedure}
\subconcept{Lemma} Suppose $D$ valid and PF dictionary and $U_1U_2...$ are iid. Then $H(W_1)=E[length(W_1)]H(U_1)$.
\subconcept{Now} nbr of bits/letter $=\frac{\lceil log|D| \rceil}{E[length(W_1)]}=\frac{\lceil log|D| \rceil}{H(W_1)}H(U_1)$.

\concept{Universal Source Coding} 
\subconcept{Lemma} If $W$ is a RV taking values in $D$ and for each $w\in D$ $q p_0 \le Pr(W=w) \le q$. Then $\lceil log|D| \rceil - log\frac{1}{p_0} \le H(W) \le \lceil log|D| \rceil$.
\subconcept{Corollary} Given $U$ and $p_U$, $\varepsilon > 0$ there exists a dictionary based code which $nbr of bits/letter \le H(U)(1+\varepsilon)$.

\concept{Lempel–Ziv method - Data compression}
\subconcept{0.} Set $D=U$
\subconcept{1.} Associate to each $w \in D$ a $\lceil log|D| \rceil$bit binary representation based on dictionry order.
\subconcept{2.} Parse the next word $w$ from the source sequence using $D$, emit the representation of $w$.
\subconcept{3.} Set $D \leftarrow (D\setminus \{w\})\cup \{wu:u \in U \}$.
\subconcept{4.} Go to 1.

\concept{Analysis of Lempel–Ziv} 
\subconcept{Technique} Compare LZ to an adversary : 
adversary knows before have $u_1u_2...$.
designs a FSM to compress this sequence.
And show LZ doas as well as the adversary.


\concept{Information-Lossless FSM Compressors}
A \subconcept{Finite-State-Machine} is 1. Set $S$ of states $|S|<\infty$.
2. Initial special state $s_0 \in S$.
3. Next state function $g:SxU \to S$.
4. $f:SxU \to \{ 0,1 \}^*$.
A \subconcept{legitimate} machine has to verify :
$\forall s \in S \forall u_1...u_m \neq v_1...v_n$ if $g(s,u_1...u_m)=g(s,v_1...v_m) \Rightarrow f(s,u_1...u_m) \neq f(s,v_1...v_n)$.
\subconcept{IL} A legitimate FSM is information-lossless.

\concept{Lower bound on the output length of an IL FSM Compressor} 
\subconcept{Fact} Suppose $m$ numbers $a_1...a_m \ge 0$ with $\sum_{i=1}^m a_i=k$.
Then, $\sum_{i=1}^m a_i log \frac{a_i}{8} \ge k log \frac{k}{8m}$ (with eq. for $a_i=\frac{k}{m}$).

\concept{LZ Compressibility of sequences} 
\subconcept{Corollary} If $u_1u_2...$ is a stationary process with entropy rate $H$ then $E[p_{LZ(U_1U_2...)}] \le H$.

\concept{Optimality of Lempel–Ziv} TODO

% -----------
\section{Channels}

\concept{Communication Channels} 
\subconcept{Def}
$P_{e,i}=Pr(U_i \neq V_i)$. 
$\bar{P_e}=\frac{1}{L} \sum_{i=1}^L Pr(U_i \neq V_i)$

\concept{Discrete Memoryless Channels} 
A channel is said to be \subconcept{memoryless} if $Pr(Y_i=y|X_i=x_i,past)=Pr(Y_i=y|X_i=x_i)$

\subconcept{Encoder} function $f:{1,...,M} \to \mathcal{X}^n$.

\subconcept{Decoder} function $\Phi:\mathcal{Y}^n \to \{1,...,M\}$.

\subconcept{Recall} $h_2(\alpha)=\alpha log( \frac{1}{\alpha})+(1-\alpha) log (\frac{1}{1-\alpha})$.

\subconcept{Theorem "Bad news"} No matter how the encoder and decoder are designed, we have $h_2(\bar{P_e})+P_e log(|U|-1) \ge \tau_s (\frac{H}{\tau_s}-\frac{C}{\tau_c})$.
Or if $R>C$ we show that any design with $rate \ge R$, we can't make bot error prob closer to 0 then some $\delta (C,R)>0$.

\subconcept{Lemma} $h_2(\frac{1}{L} \sum_{i=1}^L p_i) \ge \frac{1}{L}\sum_{i=1}^L h_2(p_i)$

\subconcept{Rate} $rate(f)=R=\frac{k}{n} \equiv R=\frac{log M}{n}$


\subconcept{Probability of error} $P_{e,i}= Pr(\Phi(Y_1...Y_n) \neq i | (X_1...X_n)=f(i))$.

\subconcept{Average prob. of error} $P_{e,avg}=\frac{1}{M}\sum_{i=1}^M P_{e,i}$.

\subconcept{Maximal prob. of error} $P_{e,max}=max_{1 \le i \le M} P_{e,i}$.


\concept{Examples of Discrete Memoryless Channels (BSC and BEC)} TODO

\concept{Transmission with or without feedback} TODO

\concept{Channel Capacity} 
$C=max_{P_X} I(X;Y)$ with $I(X;Y)=H(X)-H(X|Y)$.

\subconcept{Theorem} Given a channel $P(y|x)$ with $C=max I(X;Y)$ then any $R<C$ is achievable.

\concept{Fano's Inequality}
Suppose $U$ and $V$ RVs taking values in set $\mathcal{U}$. Let $P_{e,i}=Pr(U_i|V_i)$, then $H(U_i|V_i) \le h_2(P_{e,i})+P_{e,i} log(|\mathcal{U}|-1)$.

\subconcept{Theorem "Good news"} If $\frac{H}{\tau_s}<\frac{C}{\tau_c}$ can achieve $\bar{P_e}\to 0$. 
Or if $R<C, \varepsilon > 0$ then there exists an Enc/Dec s.t. $rate \ge R, P_{e,max}<\varepsilon$

\concept{Converse to the Channel Coding Theorem	} TODO
			
\concept{Proof of the Channel Coding Theorem} TODO

\concept{Capacity of BSC and BEC } 
\subconcept{Binary Symmetric Channel}
$\mathcal{X}=\mathcal{Y}=\{0,1\}$.
$p(y|x)= \{
   \begin{array}{l l}
      1-\delta & ,x=y \\
      \delta & ,else 
   \end{array}$
$I(X;Y)=H(Y)-H(Y|X)=H(Y)-h_2(\delta) \le 1-h_2(\delta)$.
If we choose $p_X=(\frac{1}{2},\frac{1}{2})$ then $p_Y=(\frac{1}{2},\frac{1}{2})$.
So $C=1-h_2(\delta)$ bits.

\subconcept{Binary Erasure Channel}
$\mathcal{X}=\{0,1\}$ and $\mathcal{Y}=\{0,1,e\}$.
$p(y|x)=\{
   \begin{array}{l l}
      1-\varepsilon & ,x=y \\
      \varepsilon & ,y=e 
   \end{array}$
$I(X;Y)=H(X)-H(X|Y)=(1-\varepsilon)H(X) \le 1-\varepsilon$.
If we choose $p_X=(\frac{1}{2},\frac{1}{2})$.
So $C=1-\varepsilon$ bits.

\concept{Recall convexity concavity}
$f$ is \subconcept{convex} if $\forall x_1 \in D, \forall x_2 \in D, \forall \theta \in [0,1]$
$f(\theta x_1 + (1-\theta)x_2) \le \theta f(x_1)+ (1-\theta)f(x_2)$.

$f$ is \subconcept{concave} if $\forall x_1 \in D, \forall x_2 \in D, \forall \theta \in [0,1]$
$f(\theta x_1 + (1-\theta)x_2) \ge \theta f(x_1)+ (1-\theta)f(x_2) \Leftrightarrow -f$ is convex.

$f$ is \subconcept{linear} if $\forall x_1 \in D, \forall x_2 \in D, \forall \theta \in [0,1]$
$f(\theta x_1 + (1-\theta)x_2) = \theta f(x_1)+ (1-\theta)f(x_2)$.

\subconcept{Properties}
1. If $f_1,f_2$ convex and $C_1,C_2$ non negative then $f(x)=C_1f_1(x)+C_2f_2(x)$ convex.
2. Any local minimum is a global minimum.
3. Tangent line lies below function.
4. $g$ convex and $h$ linear $\Rightarrow g(h(x))$ convex.

\concept{Jensen's Inequality }
\subconcept{Theorem} If $f$ is convex and $X$ anx RV on $D$
then $f(E[X]) \le E[f(X)]$. (idem concavity $\ge$).

\concept{Concavity of Mutual Information in Input Distribution}
\subconcept{Theorem} Fix $p(y|x)$, $I(p_X)=I(X;Y)$ is concave.
 
\concept{KKT Conditions} 
\subconcept{Def} $(P_1,...,P_k)$ satisfies KKT if $\exists \lambda$ s.t.
$\forall i, \frac{\partial f}{\partial p_i} = \lambda$ for $p_i >0$ and
$\frac{\partial f}{\partial p_i} \le \lambda$ for $p_i=0$.

\subconcept{Theorem} For concave differentiable $f(P_i)$,
$(P_1,...,P_k) \in argmax_{\sum P_i = 1 , P_i \ge 0} f(P_1,...,P_k)$
iif it satisfies the KKT conditions.

\concept{KKT Conditions (cont'd)}
\subconcept{Theorem} $p(x) \in argmax I(X;Y)$ iif $\exists \lambda$ s.t. 
$\forall x \sum_y p(y|x) log (\frac{p(y|x)}{p(y)}) \le \lambda$.
With eq. for $p(x)>0$, if so then $C=\lambda$.

\concept{Application of KKT: Capacity of Z Channel} 
\subconcept{Capacity} $C=log(1+2^{-\frac{h_2(\delta)}{1-\delta}})=\lambda$
$p_x^*(0)=1-\frac{1}{(1-\delta)(1+2^{-\frac{h_2(\delta)}{1-\delta}})}$
 
\concept{Continuous Alphabet: Differential Entropy}

\subconcept{$h(X)$} = $E[-log f_X(X)] = -\int f_X(x) log(f_X(x)) dx$.

\subconcept{$h(Y|X)$} = $-E[log f_{X|Y}(X|Y)]$.

\subconcept{$h(X,Y)$} = $E[-log f_{X Y}(X,Y)]$.

\subconcept{$D(f||g)$} = $E_f[log \frac{f_X(X)}{g_X(X)}]
= \int f_X(x) log \frac{f_X(x)}{g_X(x)} dx$.

\subconcept{$I(X;Y)$} = $D(f_{X Y}||f_X f_Y) 
= \int \int f_{X Y}(x,y) log \frac{f_{Y|X}(y|x)}{f_Y(y)} dxdy$

\concept{Properties of differential entropy}

\subconcept{1.} $I(X;Y)=h(X)-h(X|Y)=h(Y)-h(Y|X)$.

\subconcept{2.} $D(f||g) \ge 0$ with eq for $f=g$.

\subconcept{3.} $I(X;Y) \ge 0$ with eq for $X$ and $Y$ indepdt.

\subconcept{4.} Conditioning reduces $h$:
$h(X|Y) \le h(X)$ with eq for $X$ and $Y$ indepdt.

\subconcept{5.} Chain Rule : $h(X_1,...,X_n)=h(X_1)+h(X_2|X_1)+...+h(X_n|X_1...X_{n-1})$.

\subconcept{6.} $h(X_1,...,X_n) \le \sum_{i=1}^n h(X_i)$.
 
\concept{Entropy-typical sequences}
\subconcept{Discrete} $|T^n(\varepsilon)|=2^{n(H(X)+\delta(\varepsilon))}$.
\subconcept{Continuous} $Vol(T^n(\varepsilon))=2^{n(h(X)+\delta(\varepsilon))}$.
$P[X \in T^n(\varepsilon)] \xrightarrow[n \to \infty]{} 1$.

\subconcept{Theorem} $\lim_{\vartriangle \to 0} (H(X_\vartriangle) + log \vartriangle) = h(X)$.
 
\concept{Quantization} 
\subconcept{Theorem} 
$I(X_\vartriangle);Y_\vartriangle) \xrightarrow[\vartriangle \to 0]{} I(X;Y)$

\concept{Entropy of Gaussian distribution}
\subconcept{Gaussian X} We have $\mathcal{X} \sim N(0,\sigma^2)$ and $f_X(x)=\frac{1}{\sqrt{2\pi \sigma^2}} e^{-\frac{x^2}{2\sigma^2}}$.
Then $h(X)=E[-log_e f_X(X)]=\frac{1}{2} log_e(2\pi e \sigma^2) Nats 
= \frac{1}{2} log_2(2\pi e \sigma^2) Bits$.  
\subconcept{Theorem "Maximal entropy"} If $Var[X] = \sigma^2$, then 
$h(X) \le \frac{1}{2} log(2\pi e \sigma^2)$ with eq for $X \sim N(\mu,\sigma^2)$ for some $\mu$.

\concept{Capacity under cost constraint}
$C(P)=max_{f_X:E_f[b(X)] \le P} I(X;Y)$
 
\concept{Capacity of AWGN : Additive White Gaussian Noise} 
 $Y=X+Z$, $Z\sim N(0,\sigma^2)$.
 $C(P)=\frac{1}{2} log (1+\frac{P}{\sigma^2})$
 
\concept{Converse to the channel coding theorem with cost constraint}
\subconcept{Theorem} If $\frac{H}{\tau_s} > \frac{C(P)}{\tau_c}$, (any code with $\sum_{u} [p(u) \frac{1}{n} \sum_{i=1}^n b(x_{i}^{u} \le P)$) has a bit error rate $\bar{P_e} \mathbb{9} 0$ which can't be arbitrarily small. 

\subconcept{Lemma} $\sum_{i=1}^n I(X_i;Y_i) \le n C(P)$.
 
\concept{Parallel Gaussian channels (water-filling)}
With KKT $\exists \lambda$... so $P_i = max \{0,\lambda-\sigma_{i}^2\}$. Choose $\lambda$ so that $\sum_{i=1}^k P_i = P$. Indeed, $\lambda$ : waterlevel and shaded areas : $P_i$.

\concept{Proof of Channel Coding Theorem for general channels via Threshold Decoding} 
\subconcept{Theorem} $\forall R < C(P), \forall \varepsilon >0, \exists n \exists code$ with $ \forall m, \frac{1}{n} \sum_{i=1}^n b(X_{i}^{(n)}) \le P$ and $P_{max} \le \varepsilon$.
$P_{max}=max_m P[error|m]$
$P_{avg}=\frac{1}{M} \sum_{m_1}^M P[error|m]$.

\subconcept{Corollary} If $\frac{H}{\tau_s}<\frac{C(P)}{\tau_c}$ can achieve $\bar{P_e} \to 0$.

\section{Distances}
\concept{Channel Codes}
Code for \subconcept{BSC}: $M=2^{nR}$.
Given the code we see $(n,n R)$ as visible parameters.
 
\concept{Minimum Distance}
\subconcept{Hamming distance} $d_H(\bar{x},\bar{y})=\#\{i:y_i\neq x_i\}$ so maximum-likehood $\equiv$ minimum distance.
\subconcept{minimum distance of C to these parameters} $d=d_H(C)= min_{x,x' \in C, x \neq x'} d_H(x,x')$.

\subconcept{Hamming ball} with center $\bar{x}$ and radius $r$ :$B(\bar{x},r)=\{\bar{y} \in \{0,1\}^n : d_H(\bar{y},\bar{x}) \le r\}$.
Also note $|B(\bar{x},r)|=1+\binom{n}{1}+...+\binom{n}{r}$

\subconcept{Theorem} Given $\bar{x},\bar{y},\bar{z} \in \{0,1\}^n$.
$d_H(\bar{x},\bar{y}) \ge 0$ with eq if $\bar{x}=\bar{y}$.
$d_H(\bar{x},\bar{y}) = d_H(\bar{y},\bar{x})$.
$d_H(\bar{x},\bar{z}) \le d_H(\bar{x},\bar{y}) + d_H(\bar{y},\bar{z})$.

\subconcept{Corollary} Suppose $\bar{x},\bar{x'} \in \{0,1\}^n$ with $d=d_H(\bar{x},\bar{x'})$. Set $r=\lfloor \frac{d-1}{2} \rfloor$.
Consider $B(\bar{x},r)$ and $B(\bar{x'},r)$ then they are disjoint.

\concept{Singleton Bound (Bad news)}
If $\# of codewords = M > 2^k$ then $d_{min} \le n-k$.
 
\concept{Sphere-packing Bound (Bad news)}
Suppose $C \subset \{0,1\}^n$ is a binary code with $M$ codewords and $d=d_{min}$.
Then, $M \left[ \sum_{i=0}^{r=\lfloor \frac{d-1}{2} \rfloor} \binom{n}{i} \right] \le 2^n $.

\concept{Gilbert–Varshamov Bound (Good news)}
For every $(n,M,d)$ satisfy
$d_{min}=d$ and $M \left[ \sum_{i=0}^{d-1} \binom{n}{i} \right] \ge 2^n $
there exists a code $C \subset \{0,1\}^n$ with $\ge M$ codewords and $d_{min}(C) \ge d$.

\concept{Linear Codes}
A code $C \subset \{0,1\}^n$ is said to be \subconcept{linear} if $x,y \in C$ then $x+y \in C \equiv C$ is a vector space in $\{0,1\}^n$.
 
\concept{Generator Matrix}
\subconcept{Fact 1} If $C$ is a linear code, then $|C|=2^k$ for some int $k$ and there exists a $k\times n$ binary matrix $G$ s.t.
$C=\{[u_1...u_k] G:(u_1...u_k) \in \{0,1\}^k\}$
$\equiv C$ is a raw space of $G$.

\subconcept{Fact 2} If $C$ is linear, say with generator matrix 
$G(k \times n)$ the enc. doesn't need to store all $2^k$ codewords.
 
\concept{Parity-check Matrix} TODO
 
\concept{Hamming Codes} TODO

\concept{Reed–Solomon Codes} TODO

\concept{Polar Codes} TODO

% ---------- Page break
\ifdefined \longversion % Nothing
\else \newpage
\fi



% ---------- Credits
\section{Credits}
Most content taken from the lecture notes of Emre Telatar's \href{http://ipg.epfl.ch/doku.php?id=en:courses:2015-2016:itc}{Information Theory and Coding class} at EPFL, 2015.

% ---------- Footer
\vspace{0.5cm}
\hrule
\vspace{0.5cm}
\tiny
Rendered \today. Written by Lucile Madoulaud.

\end{multicols*}
\end{document}
